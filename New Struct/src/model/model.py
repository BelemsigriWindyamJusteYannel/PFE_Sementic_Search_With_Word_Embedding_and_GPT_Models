import chromadb,requests,os
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# üîπ Chargement du mod√®le GGUF Mistral avec llama-cpp
model_path = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
model = Llama(model_path=model_path)

from sentence_transformers import SentenceTransformer, util
# 
semantic_model = SentenceTransformer("BAAI/bge-m3")

def generate_final_answer(question, best_responses):
    """ G√©n√®re une r√©ponse bien structur√©e en utilisant Llama/Mistral. """
    context = "\n".join(best_responses)
    
    prompt = (
        f"Tu es un assistant au service de scolarit√© de l √©cole sup√©rieure de technologie de f√®s"
        f"Vous √™tes un assistant qui r√©pond aux questions bas√©es sur un texte r√©glementaire."
        f"Voici un texte. R√©pondez √† la question suivante en utilisant ce contexte : {context}\n\nQuestion : {question}\nR√©ponse :"
    )

    output = model(prompt, max_tokens=200, temperature=1)
    return output["choices"][0]["text"].strip()



def get_model_response(query):
    
    db_path = os.path.abspath("./../data/chromadb_data_base")
    client = chromadb.PersistentClient(path=db_path)
    collection = client.get_collection(name="embeddings_collection")
    print(collection)
    query_embedding = semantic_model.encode(query)

    # Recherche dans ChromaDB
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=5 # Nombre de r√©sultats √† retourner
    )

    response = generate_final_answer(query, results['documents'][0])
    
    return response
        



# get_model_response()